{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Module 1: Machine Learning**\n",
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "Performance metrics are crucial for evaluating how well a machine learning model performs. The specific metrics you'll use depend on the type of machine learning task you're tackling. Here's a breakdown of common metrics for different tasks. Choosing the right metrics depends on your specific problem and what aspects of model performance are most important:\n",
    "\n",
    "### Classification Tasks:  \n",
    "- #### Accuracy\n",
    "Measures the overall proportion of correct predictions.  \n",
    "\n",
    "In machine learning, accuracy for classification tasks is a metric that tells you, in simple terms, how often your model predicts the correct class for a given data point. It's calculated as the number of correct predictions divided by the total number of predictions.\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "**Correct Predictions:** These are the instances where the model assigns the same class label to a data point as the actual class it belongs to.  \n",
    "**Total Predictions:** This refers to every single prediction made by the model for all the data points.  \n",
    "Imagine you have a model that classifies emails as spam or not spam. Accuracy tells you what percentage of emails the model classified correctly (both spam and not spam).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "You have a dataset of 100 emails.  \n",
    "Your model makes predictions for all 100.  \n",
    "Out of those 100 predictions, 85 are correct (correctly classified as spam or not spam).  \n",
    "**Accuracy = (Number of Correct Predictions) / (Total Predictions) = 85 / 100 = 0.85**  \n",
    "\n",
    "Therefore, your model has an accuracy of 85%.\n",
    "\n",
    "While accuracy is a straightforward metric, it's important to consider its limitations:\n",
    "\n",
    "**Imbalanced Datasets:** If your dataset has a heavy skew towards one class (e.g., mostly not spam emails), a model might achieve high accuracy by simply predicting the majority class all the time. This wouldn't be a good model in practice.  \n",
    "**Doesn't distinguish between types of errors:** Accuracy doesn't tell you what kind of mistakes the model is making. Misclassifying a critical case (e.g., missing a spam email) might be much more important than misclassifying a harmless one.\n",
    "\n",
    "That's why alongside accuracy, data scientists often use other metrics like precision, recall, and F1-score to get a more nuanced understanding of the model's performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- #### Confusion Matrix\n",
    "Visualizes the model's performance on different classes, highlighting errors.  \n",
    "\n",
    "A confusion matrix, unlike accuracy, provides a more fine-grained look at your machine learning model's performance in a classification task. It's not a single number but rather a table that visually summarizes how many data points fall into each category of prediction outcomes.\n",
    "\n",
    "Here's how a confusion matrix works:\n",
    "\n",
    "**Rows represent the actual classes** your data points belong to.  \n",
    "**Columns represent the predicted classes** by your model.  \n",
    "The table cells are then populated with counts based on these categories:\n",
    "\n",
    "**True Positives (TP):** These are the cases where the model correctly predicted the positive class.  \n",
    "**True Negatives (TN):** These are the cases where the model correctly predicted the negative class.  \n",
    "**False Positives (FP):** These are the errors where the model predicted positive when the actual class was negative (also known as Type I error).  \n",
    "**False Negatives (FN):** These are the errors where the model predicted negative when the actual class was positive (also known as Type II error).  \n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's say you're training a model to classify images as containing cats or dogs. Here's a possible confusion matrix:\n",
    "\n",
    "| Predicted Class |\tCat\t| Dog | Total |\n",
    "| --------------- | --- | --- | ----- |\n",
    "| Cat (Actual) | TP\t| FN | TP + FN |\n",
    "| Dog (Actual) | FP\t| TN | FP + TN |\n",
    "| Total\t| TP + FP | FN + TN | Total Data Points |\n",
    "\n",
    "\n",
    "**Benefits of using a confusion matrix:**\n",
    "\n",
    "**Visualization:** It gives you a clear visual representation of how your model is performing for each class, allowing you to identify areas for improvement.  \n",
    "**Identifying Errors:** By looking at high values in FN (missed positive cases) or FP (incorrect positive cases), you can understand what kind of errors the model is making the most.  \n",
    "**Imbalanced Datasets:** Accuracy can be misleading for imbalanced datasets. A confusion matrix helps you assess performance for each class independently.  \n",
    "\n",
    "**Remember:** A confusion matrix doesn't provide a single performance score.  You can, however, calculate metrics like precision, recall, and F1-score based on the values in the confusion matrix to get more specific insights.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- #### Precision\n",
    "Measures the proportion of positive predictions that are actually correct.  \n",
    "\n",
    "In machine learning classification tasks, precision tells you specifically about the quality of your model's positive predictions. It essentially answers the question: **Out of all the instances where the model predicted a positive class, how many were actually correct?**\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "**Positive Class:** This refers to the class you're particularly interested in identifying correctly. For instance, in a spam detection model, the positive class would be \"spam.\"  \n",
    "**Positive Predictions:** These are all the instances where the model predicted the data point belongs to the positive class.  \n",
    "\n",
    "**Precision is calculated as:**\n",
    "\n",
    "`Precision = (True Positives) / (True Positives + False Positives)`  \n",
    "**Let's understand this with an example:**\n",
    "\n",
    "Imagine you're building a system to classify loan applications as approved or rejected. Here's how precision can be interpreted:\n",
    "\n",
    "**Scenario:** Your model predicts 80 loan applications to be approved (positive predictions). Out of those 80, 60 actually turn out to be good applicants (true positives). The remaining 20 were mistakenly classified as good (false positives).  \n",
    "**Precision:** (True Positives) / (True Positives + False Positives) = 60 / (60 + 20) = 0.75  \n",
    "Therefore, your model has a precision of 75% for approved loans. In other words, 75% of the time the model predicts an approval, it's actually a good applicant.\n",
    "\n",
    "**Here's what high and low precision mean:**\n",
    "\n",
    "**High Precision (close to 1):** This indicates your model is good at filtering out irrelevant data and making precise positive predictions. There are very few false positives.  \n",
    "**Low Precision (close to 0):** This suggests the model is making a lot of false positive predictions. Many of the positive predictions it makes are actually incorrect.  \n",
    "**Consideration:**\n",
    "\n",
    "Precision is particularly important when the cost of misclassifying a positive case is high. For instance, in a medical diagnosis system, a false positive (predicting a disease when it's absent) can lead to unnecessary procedures or anxiety.  \n",
    "**Remember:** Precision should ideally be used in conjunction with other metrics like recall (ability to identify all positive cases) to get a complete picture of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Recall:\n",
    "Measures the proportion of actual positive cases the model correctly identified.  \n",
    "\n",
    "Recall, along with precision,  is a crucial metric for evaluating your machine learning model's performance in classification tasks. It focuses on a different aspect compared to precision and asks the question: **Out of all the actual positive cases in your data, how many did your model correctly identify?**\n",
    "\n",
    "Here's a breakdown of recall:\n",
    "\n",
    "**Actual Positive Cases:** These are the data points that genuinely belong to the positive class you're interested in.  \n",
    "**True Positives:** These are the instances where the model correctly predicted the data point belongs to the positive class.  \n",
    "\n",
    "**Recall is calculated as:**\n",
    "\n",
    "`Recall = (True Positives) / (True Positives + False Negatives)`\n",
    "\n",
    "**Understanding Recall with an Example:**\n",
    "\n",
    "Imagine you're training a model to identify fraudulent transactions on a credit card. Here's how recall can be interpreted:  \n",
    "\n",
    "**Scenario:** There are 100 actual fraudulent transactions (positive cases) in your data. Your model correctly identifies 80 of them (true positives). The remaining 20 fraudulent transactions are missed by the model (false negatives).  \n",
    "**Recall:** (True Positives) / (True Positives + False Negatives) = 80 / (80 + 20) = 0.8  \n",
    "Therefore, your model has a recall of 80% for fraudulent transactions. In other words, it catches 80% of the actual fraudulent transactions happening.  \n",
    "\n",
    "**Here's what high and low recall mean:**\n",
    "\n",
    "**High Recall (close to 1):** This indicates your model is good at capturing most of the actual positive cases. There are very few false negatives.  \n",
    "**Low Recall (close to 0):** This suggests the model is missing a significant portion of the actual positive cases (high number of false negatives).\n",
    "\n",
    "**Consideration:**\n",
    "\n",
    "Recall is particularly important when the cost of missing a positive case is high. For instance, in an email spam filter, a false negative (missing a spam email) can lead to important information being overlooked.\n",
    "\n",
    "**Remember:**  Just like precision, recall is best used in conjunction with other metrics like precision to get a well-rounded understanding of your model's performance.  In some cases, you might even consider using the F1-score, which is a harmonic mean of precision and recall, to balance these two metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### F1-Score\n",
    "A harmonic mean of precision and recall, useful for imbalanced datasets.  \n",
    "\n",
    "The F1-score, also known as F-measure, tackles a challenge that arises when using precision and recall independently in machine learning classification tasks. Precision tells you about the quality of your positive predictions, while recall focuses on how well you capture all the actual positive cases. But what if these two metrics disagree?\n",
    "\n",
    "The F1-score provides a balanced view by taking the **harmonic mean** of precision and recall. The harmonic mean, unlike the arithmetic mean (average), punishes extreme values (very high or very low) more harshly. Here's how it works:\n",
    "\n",
    "`F1 Score = 2 * (Precision * Recall) / (Precision + Recall)`\n",
    "\n",
    "**Why F1-Score is Useful:**\n",
    "\n",
    "**Balances Precision and Recall:** If a model has high precision but low recall (great at some positive predictions but misses many others), or vice versa, the F1-score will reflect this by giving a more balanced assessment.  \n",
    "**Useful for Imbalanced Datasets:** In datasets where one class is much more frequent (e.g., mostly non-spam emails), a model might achieve high accuracy by simply predicting the majority class all the time. F1-score helps address this by considering both precision and recall.\n",
    "\n",
    "**Interpreting F1-Score:**\n",
    "\n",
    "**F1 close to 1:** Excellent performance, the model achieves both high precision and recall.  \n",
    "**F1 close to 0:** Poor performance, the model struggles to identify positive cases accurately.  \n",
    "\n",
    "**Here's a caveat to consider:**\n",
    "\n",
    "**Choosing the right weights:** There are variations of F1-score where you can assign different weights to precision and recall based on your specific problem. For instance, if missing a fraudulent transaction is much more critical than accidentally flagging a valid one, you might choose a higher weight for recall in the F1-score calculation.  \n",
    "\n",
    "**In conclusion,** the F1-score is a valuable metric for evaluating classification models, especially when precision and recall are both important and the dataset might be imbalanced. It provides a single score that considers both aspects, giving you a more balanced view of the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### AUC-ROC:\n",
    "Area Under the Curve of the Receiver Operating Characteristic, evaluates the model's ability to distinguish between classes.\n",
    "\n",
    "AUC-ROC, is a metric used to evaluate the performance of binary classification models. It combines two key concepts:\n",
    "\n",
    "**ROC Curve (Receiver Operating Characteristic Curve):** This curve visually depicts the model's performance at various classification thresholds. It plots the True Positive Rate (TPR) on the y-axis against the False Positive Rate (FPR) on the x-axis. As the classification threshold is varied, the TPR and FPR change, and the ROC curve traces these changes.  \n",
    "**Area Under the Curve (AUC):** This is a single numerical value between 0 and 1 that summarizes the overall performance of the model across all possible classification thresholds. It essentially represents the proportion of the ROC curve that lies above the random chance line (diagonal line from (0,0) to (1,1)).\n",
    "\n",
    "**Here's a breakdown of what AUC-ROC tells you:**\n",
    "\n",
    "**Higher AUC (closer to 1):** Indicates a better model that can effectively distinguish between positive and negative classes across all thresholds. The ROC curve will be closer to the upper left corner of the graph.  \n",
    "**Lower AUC (closer to 0):** Indicates a poor model that performs no better than random guessing. The ROC curve will be closer to the diagonal line.\n",
    "AUC of 0.5: Represents random chance classification, where the model is essentially flipping a coin for its predictions.\n",
    "\n",
    "**Benefits of using AUC-ROC:**\n",
    "\n",
    "**Threshold Agnostic:** Unlike accuracy, which is tied to a specific classification threshold, AUC-ROC considers the model's performance across all thresholds. This is useful because you might not always have a pre-defined threshold in practice.  \n",
    "**Robust to Imbalanced Datasets:** Accuracy can be misleading in datasets with a skewed class distribution. AUC-ROC is less affected by this issue.  \n",
    "\n",
    "**In conclusion,** AUC-ROC is a powerful metric for evaluating how well your binary classification model differentiates between positive and negative classes. It provides a comprehensive understanding of the model's performance across different thresholds, making it a valuable tool for machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Tasks:\n",
    "- #### Mean Absolute Error (MAE)\n",
    "Average of the absolute differences between predicted and actual values.  \n",
    "\n",
    "Mean Absolute Error (MAE) is a common metric used to evaluate the performance of regression models. In simpler terms, it tells you how far off your model's predictions are from the actual values on average, in terms of absolute difference.\n",
    "\n",
    "Here's a breakdown of MAE:\n",
    "\n",
    "**Absolute Difference:** This refers to the non-negative difference between a predicted value and the corresponding actual value. For example, if the model predicts a value of 10 but the actual value is 15, the absolute difference is 5 (|15 - 10| = 5).  \n",
    "**Mean:** This refers to the average of all the absolute differences calculated for each data point in your test set.\n",
    "\n",
    "**MAE is calculated as:**\n",
    "\n",
    "`MAE = (1/n) * Σ |y_i - ŷ_i|` \n",
    "Where:\n",
    "\n",
    "- n = total number of data points  \n",
    "- y_i = actual value for data point i  \n",
    "- ŷ_i = predicted value for data point i by the model\n",
    "- Σ = summation over all data points\n",
    "\n",
    "**Interpreting MAE:**\n",
    "\n",
    "**Lower MAE:** Indicates a better model, with predictions generally closer to the actual values on average.  \n",
    "**Higher MAE:** Indicates a worse model, with predictions deviating more from the actual values on average.  \n",
    "\n",
    "**Here are some advantages of using MAE:**\n",
    "\n",
    "**Easy to Understand:** MAE is a straightforward metric. The units of MAE are the same as the units of your target variable, making it easy to interpret the magnitude of the errors.  \n",
    "**Less Sensitive to Outliers:** Compared to some other metrics like Mean Squared Error (MSE), MAE is less affected by outliers in your data. Outliers are data points that fall far away from the majority of the data. Since MAE uses absolute differences, extreme outliers won't significantly inflate the error score as much as they would in MSE (which squares the errors).  \n",
    "\n",
    "**However, MAE also has a limitation:**\n",
    "\n",
    "**Doesn't Consider Direction of Errors:** MAE only considers the magnitude of the errors (how far off the predictions are), not their direction. A positive prediction that's far off by 5 units is treated the same as a negative prediction that's far off by 5 units. This might not be ideal in some cases where underestimating or overestimating can have different consequences.\n",
    "\n",
    "**In conclusion,** Mean Absolute Error (MAE) is a valuable metric for regression tasks. It provides a clear and interpretable measure of how close your model's predictions are to the actual values, on average. While it doesn't consider the direction of errors, its simplicity and robustness to outliers make it a popular choice for evaluating regression model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Mean Squared Error (MSE)\n",
    "Average of the squared differences between predicted and actual values (sensitive to outliers).  \n",
    "\n",
    "Mean Squared Error (MSE) is another widely used metric for evaluating the performance of regression models. Compared to Mean Absolute Error (MAE), which focuses on the average absolute difference between predictions and actual values, MSE squares these differences before averaging.\n",
    "\n",
    "Here's a breakdown of MSE:\n",
    "\n",
    "**Squared Differences:** For each data point, the difference between the predicted value (ŷ_i) and the actual value (y_i) is squared. Squaring emphasizes larger errors more heavily in the calculation.  \n",
    "**Mean:** The squared differences are then averaged across all data points in your test set.\n",
    "\n",
    "**MSE is calculated as:**\n",
    "\n",
    "`MSE = (1/n) * Σ (y_i - ŷ_i)^2``\n",
    "\n",
    "Where:\n",
    "\n",
    "- n = total number of data points  \n",
    "- y_i = actual value for data point i\n",
    "- ŷ_i = predicted value for data point i by the model\n",
    "- Σ = summation over all data points\n",
    "\n",
    "**Interpreting MSE:**\n",
    "\n",
    "**Lower MSE:** Indicates a better model, with predictions generally closer to the actual values on average. Since squares of larger errors contribute more to the MSE, a lower MSE reflects smaller prediction errors overall.  \n",
    "**Higher MSE:** Indicates a worse model, with predictions deviating more from the actual values on average, and potentially larger errors.\n",
    "\n",
    "**Here's why MSE is used:**\n",
    "\n",
    "**Focuses on Larger Errors:** Squaring the errors gives more weight to significant prediction mistakes. This can be useful when large errors are more detrimental to your model's performance.  \n",
    "**Enables Easier Calculus:** MSE is mathematically convenient for optimization algorithms used in training machine learning models. Many optimization techniques rely on minimizing a cost function, and the squared nature of MSE makes it easier to work with mathematically.\n",
    "\n",
    "**However, MSE also has some drawbacks:**\n",
    "\n",
    "**Sensitive to Outliers:** Outliers in your data can significantly inflate the MSE because their squared errors will contribute more heavily to the overall value. This can be misleading if you have a few extreme data points.  \n",
    "**Units are Squared:** The units of MSE are the square of the units of your target variable. This can make interpreting the magnitude of the error less intuitive compared to MAE.\n",
    "\n",
    "**In conclusion,** Mean Squared Error (MSE) is a popular metric for regression tasks, particularly when larger errors are more concerning. It emphasizes these larger errors by squaring the differences between predictions and actual values. However, its sensitivity to outliers and units being squared are limitations to consider when evaluating your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- #### Root Mean Squared Error (RMSE)\n",
    "Square root of MSE, in the same units as the data.  \n",
    "\n",
    "Root Mean Squared Error (RMSE), as the name suggests, is closely tied to the concept of Mean Squared Error (MSE) discussed earlier. It's a metric used in regression tasks to evaluate how well your model's predictions match the actual values. However, unlike MSE, RMSE doesn't directly output squared error units.\n",
    "\n",
    "Here's how RMSE builds upon MSE:\n",
    "\n",
    "**Mean Squared Error (MSE):** As you know, MSE takes the average of the squared differences between predicted and actual values for all data points.  \n",
    "**Taking the Square Root:** RMSE simply takes the square root of the MSE.  \n",
    "\n",
    "**Formally, RMSE is calculated as:**\n",
    "\n",
    "`RMSE = √(MSE) = √((1/n) * Σ (y_i - ŷ_i)²)``\n",
    "\n",
    "Where:\n",
    "\n",
    "- n = total number of data points\n",
    "- y_i = actual value for data point i\n",
    "- ŷ_i = predicted value for data point i by the model\n",
    "- Σ = summation over all data points\n",
    "\n",
    "**Why use RMSE?**\n",
    "\n",
    "**Units in Original Scale:** Since RMSE is the square root of MSE, it has the same units as the original target variable you're trying to predict. This makes interpreting the error magnitude more intuitive compared to MSE, which uses squared units.  \n",
    "**Relates to MAE:** RMSE can be thought of as a scaled version of Mean Absolute Error (MAE). While MAE gives the average absolute difference between predictions and actual values, RMSE takes the square root of the average squared difference (MSE), essentially putting it on a similar scale as MAE.  \n",
    "\n",
    "**In essence, RMSE offers the following benefits:**\n",
    "\n",
    "**Interpretability:** Units are in the same scale as the target variable, making it easier to understand the error magnitude.  \n",
    "**Connection to MAE:** Relates to the more intuitive MAE metric.  \n",
    "\n",
    "**However, RMSE also inherits a limitation from MSE:**\n",
    "\n",
    "**Sensitivity to Outliers:** Like MSE, RMSE can be significantly influenced by outliers in your data because their squared errors are amplified.  \n",
    "\n",
    "**In conclusion,** Root Mean Squared Error (RMSE) is a valuable metric for regression tasks. It provides an interpretable measure of the average error between predictions and actual values, in the same units as your target variable. While it shares the limitation of being sensitive to outliers with MSE, its interpretability and connection to MAE make it a popular choice for evaluating model performance in regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- #### R-Squared (R²)\n",
    "Proportion of variance in the dependent variable explained by the model.\n",
    "\n",
    "R-Squared (often denoted by R²), also known as the coefficient of determination, is a metric used in regression tasks to assess how well your model explains the variance in the dependent variable (what you're trying to predict). It essentially captures the proportion of variance explained by your model compared to the total variance in the data.\n",
    "\n",
    "Here's a breakdown of R-Squared:\n",
    "\n",
    "Variance: In statistics, variance represents the average squared difference from the mean. It tells you how spread out the data points are from the average value.\n",
    "Explained Variance: This refers to the portion of the variance in the dependent variable that can be explained by the independent variables (features) used in your model. In simpler terms, it's the variance captured by the fitted regression line.\n",
    "Total Variance: This refers to the total variability in the dependent variable itself, considering how spread out the data points are from the mean regardless of the model.\n",
    "R-Squared is calculated as:\n",
    "\n",
    "R² = 1 - (Σ (y_i - ŷ_i)²) / (Σ (y_i - ȳ)²)\n",
    "Where:\n",
    "\n",
    "y_i = actual value for data point i\n",
    "ŷ_i = predicted value for data point i by the model\n",
    "ȳ = mean of the actual values\n",
    "Σ = summation over all data points\n",
    "Interpreting R-Squared:\n",
    "\n",
    "Higher R² (closer to 1): Indicates a better model, with a larger proportion of the variance in the dependent variable explained by the model.\n",
    "Lower R² (closer to 0): Indicates a weaker model, with a smaller proportion of the variance explained by the model. A value of 0 means the model explains no variance, essentially performing no better than predicting the average value (ȳ) for all data points.\n",
    "Here are some key points to remember about R-Squared:\n",
    "\n",
    "Proportion of Explained Variance: It tells you the percentage of variance explained by the model, not the absolute error between predictions and actual values.\n",
    "Doesn't Guarantee Good Predictions: A high R² doesn't necessarily mean your model makes good predictions, especially if your data has inherent biases or there's a poor model fit. It's crucial to visualize the data and residuals (differences between actual and predicted values) to check for these issues.\n",
    "Beware of Overfitting: A very high R² might suggest overfitting, where the model captures too much noise in the data and might not perform well on unseen data.\n",
    "In conclusion, R-Squared is a common metric for regression tasks that provides insights into how well your model explains the variance in the dependent variable. While a higher R² is generally desirable, it's important to consider other factors like model fit and potential for overfitting for a more comprehensive evaluation of your regression model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
