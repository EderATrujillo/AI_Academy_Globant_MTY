{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Module 1: Machine Learning**\n",
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Data preprocessing is a crucial step in the machine learning pipeline, involving the preparation and transformation of raw data into a format suitable for building and training models. Effective preprocessing ensures that the data is clean, relevant, and ready for analysis, ultimately leading to better model performance. Data preprocessing involves transforming raw data into a clean and usable format. This includes handling missing data, normalization, standardization, and encoding categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps in Data Preprocessing:\n",
    "**1. Data Collection:**\n",
    "- Gathering raw data from various sources, such as databases, files, APIs, and sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "#data_url = 'https://example.com/dataset.csv'\n",
    "#df = pd.read_csv(data_url)\n",
    "\n",
    "data = {'feature1': [1, 4, 7],\n",
    "        'feature2': [2, 5, 8],\n",
    "        'feature3': [3, 6, 9],\n",
    "        'target': [1, 1, 0]}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**2. Data Cleaning:**\n",
    "- Handling Missing Values: Techniques include imputation (mean, median, mode, or using algorithms), or removing rows/columns with missing values.\n",
    "- Removing Duplicates: Identifying and removing duplicate entries to avoid bias.\n",
    "- Handling Outliers: Detecting and managing outliers, either by removal or transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "df.fillna(df.mean(), inplace=True)  # Imputation with mean\n",
    "\n",
    "# Removing duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Handling outliers (example using z-score method)\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "z_scores = np.abs(stats.zscore(df.select_dtypes(include=[np.number])))\n",
    "df = df[(z_scores < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**3. Data Integration:**\n",
    "- Combining data from different sources into a single cohesive dataset. This may involve merging tables, joining data, and ensuring consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming another dataframe df2 to merge\n",
    "\n",
    "data2 = {'feature1': [1, 4, 9],\n",
    "        'feature2': [2, 5, 8],\n",
    "        'feature3': [3, 6, 7],\n",
    "        'target': [1, 1, 0]}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Merging dataframes on a common key\n",
    "#mergedData = pd.merge(df, df2)\n",
    "mergedData = pd.merge(df, df2, on='feature1')\n",
    "print(mergedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**4. Data Transformation:**\n",
    "- Normalization/Standardization: Scaling features to a standard range, such as [0, 1] or to have mean 0 and variance 1.\n",
    "- Encoding Categorical Variables: Converting categorical data into numerical format using techniques like one-hot encoding, label encoding, or binary encoding.\n",
    "- Feature Engineering: Creating new features from existing data to enhance model performance.\n",
    "- Dimensionality Reduction: Reducing the number of features using techniques like PCA, LDA, or t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Create transformers for preprocessing\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Apply transformations\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "print(X_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**5. Data Splitting:**\n",
    "- Dividing the dataset into training, validation, and test sets. Common splits are 70-20-10 or 80-20 for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps help in transforming raw data into a format that enhances the performance and accuracy of machine learning models. By using libraries like Pandas and Scikit-learn, these tasks can be efficiently handled in Python. Combining all the steps into a cohesive pipeline (please notice the load of the data set changed from pulling it from a URL to setting up manually for ease the running of the program):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "#data_url = 'https://example.com/dataset.csv'\n",
    "#df = pd.read_csv(data_url)\n",
    "\n",
    "data = {'feature1': [1, 4, 7],\n",
    "        'feature2': [2, 5, 8],\n",
    "        'feature3': [3, 6, 9],\n",
    "        'target': [1, 1, 0]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Data cleaning\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Create transformers for preprocessing\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline that includes preprocessing and model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
