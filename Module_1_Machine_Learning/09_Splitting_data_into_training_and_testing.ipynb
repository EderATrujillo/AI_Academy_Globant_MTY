{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data into Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "- Understand why and how to split datasets into training and testing sets, and training, validation and testing sets.\n",
    "- Understand some technical nuances on splitting datasets such as reproducibility and how to deal with imbalanced datasets.\n",
    "- Implement dataset splits in Python with `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data into Training and Testing datasets\n",
    "\n",
    "When training a model we want to separate concerns in our datasets. We need a portion for our dataset to be the data the model trains on, this set is usually known as **training set**. We also need another portion of our data to evaluate the performance of our model (here we understand _performance_ as _some measure of how good the predictions of our model are_). This second dataset is known as **testing dataset**. We don't want these sets to be overlapping as this is a source of overfitting. In practice the proportions of these splits are around **70-30** to **90-10** percent (**train-test**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.0-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (2.0.0)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.13.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\eder.trujillo\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\eder.trujillo\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached scikit_learn-1.5.0-cp312-cp312-win_amd64.whl (10.9 MB)\n",
      "Using cached pandas-2.2.2-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached scipy-1.13.1-cp312-cp312-win_amd64.whl (45.9 MB)\n",
      "Installing collected packages: scipy, joblib, scikit-learn, pandas\n",
      "Successfully installed joblib-1.4.2 pandas-2.2.2 scikit-learn-1.5.0 scipy-1.13.1\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "!pip install scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making random splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "wine_ds = load_wine()\n",
    "wine_X = wine_ds.data\n",
    "wine_y = wine_ds.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine_X,\n",
    "                                                    wine_y,\n",
    "                                                    train_size=TRAIN_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a seed for reproducibility\n",
    "\n",
    "When doing splits, we might want to ensure subsequent runs of the same training pipelines yield the same result. For doing that we want to make our split consistent across runs. Scikit-learn `train_test_split` function has an optional parameter for a \"random state\" which ensures the data split to be the same across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "TRAIN_SIZE = 0.8\n",
    "RANDOM_SEED = 314159\n",
    "\n",
    "wine_ds = load_wine()\n",
    "wine_X = wine_ds.data\n",
    "wine_y = wine_ds.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine_X,\n",
    "                                                    wine_y,\n",
    "                                                    train_size=TRAIN_SIZE,\n",
    "                                                    random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into training, testing, and validation\n",
    "\n",
    "We can include additional validation of our model during the training phase. This can help us reduce overfitting. For this purpose we include a third split, which is usually known as a **validation set**. This set usually consists of **40** to **60** percent of the data that would be reserved for the testing set. The three of these sets should not be overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "TRAIN_SIZE = 0.8\n",
    "TEST_VAL_RATIO = 0.5\n",
    "RANDOM_SEED = 314159\n",
    "\n",
    "wine_ds = load_wine()\n",
    "wine_X = wine_ds.data\n",
    "wine_y = wine_ds.target\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(wine_X,\n",
    "                                                    wine_y,\n",
    "                                                    train_size=TRAIN_SIZE,\n",
    "                                                    random_state=RANDOM_SEED)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp,\n",
    "                                                y_temp,\n",
    "                                                train_size=TEST_VAL_RATIO,\n",
    "                                                random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling imbalanced data or time-series\n",
    "\n",
    "When handling highly imbalanced(skewed) data, it is possible that some data class gets misrepresented in either the training or testing/validation sets. For instance, if we have a dataset for fraud occurrences, we might have a low percent of fraud events, which might not make it to the testing dataset. Other kind of consideration when splitting data might be when working with time-based events, for instance when dealing with forecasting, we would like to have data around the same time points in all of our dataset splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Split\n",
    "\n",
    "A stratified split ensures class distribution in classification models remains consistent across dataset splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "TRAIN_SIZE = 0.8\n",
    "TEST_VAL_RATIO = 0.5\n",
    "RANDOM_SEED = 314159\n",
    "\n",
    "wine_ds = load_wine()\n",
    "wine_X = wine_ds.data\n",
    "wine_y = wine_ds.target\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(wine_X,\n",
    "                                                    wine_y,\n",
    "                                                    train_size=TRAIN_SIZE,\n",
    "                                                    random_state=RANDOM_SEED,\n",
    "                                                    stratify=wine_y)\n",
    "# We want to use target columns as these mark our classes in a classification model\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp,\n",
    "                                                y_temp,\n",
    "                                                train_size=TEST_VAL_RATIO,\n",
    "                                                random_state=RANDOM_SEED,\n",
    "                                                stratify=y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-series splits\n",
    "\n",
    "When working with time series, a method to split data consists in:\n",
    "- Splitting first the data into segments corresponding a time period.\n",
    "- Create splits for all of these segments.\n",
    "- Join resulting datasets.\n",
    "The `scikit-learn` library also contains a helper module for this kind of split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "bike_sharing = fetch_openml(\"Bike_Sharing_Demand\", version=2, as_frame=True)\n",
    "bike_df = bike_sharing.frame\n",
    "# We retrieve a bike sharing dataset from OpenML repository.\n",
    "bike_y = bike_df[\"count\"]\n",
    "# We use the quantity of rentals as labels.\n",
    "bike_X = bike_df.drop(\"count\", axis=\"columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m tscv \u001b[38;5;241m=\u001b[39m TimeSeriesSplit(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m tscv\u001b[38;5;241m.\u001b[39msplit(bike_X):\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(bike_X[train_index])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, test_index in tscv.split(bike_X):\n",
    "    X_train.add(bike_X[train_index])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "\n",
    "- https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
