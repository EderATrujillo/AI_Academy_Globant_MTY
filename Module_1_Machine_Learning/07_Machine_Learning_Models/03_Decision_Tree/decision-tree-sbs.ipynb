{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decision Trees**\n",
    "\n",
    "Decision trees are predictive models that use a set of rules based on data characteristics to make decisions.\n",
    "\n",
    "A decision tree is a hierarchical structure where each node represents a feature (or attribute) of the data, each branch represents an outcome of that feature, and each leaf represents a class label or a prediction value.\n",
    "\n",
    "`USE CASE EXAMPLES`\n",
    "\n",
    "Purchase Probability:<br>\n",
    "Variables: Income, Age, Gender<br>\n",
    "Objective: Determine whether a person will make a certain purchase or not.<br>\n",
    "\n",
    "    A[Income > $70,000] -->|Yes| B[Age]\n",
    "    B -->|â‰¥ 40| C[Purchase: Yes]\n",
    "    B -->|< 40| D[Purchase: No]\n",
    "    A -->|No| E[Has Bought Before]\n",
    "    E -->|Yes| F[Will Purchase: Yes]\n",
    "    E -->|No| G[Will Purchase: No]\n",
    "\n",
    "\n",
    "`Income:` used as the first decision criterion.<br>\n",
    "If income is higher than $70,000, the decision tree relies on age. If not, it relies on gender.<br>\n",
    "\n",
    "`Age:` determines the probability of purchase for customers of a certain age.<br>\n",
    "\n",
    "`Has Bought Before:` determines the probability of purchase for customers who have previously bought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install numpy pandas matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating synthetic data for market segmentation\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "# Factors to consider for market segmentation\n",
    "# - Age: between 18 and 70 years\n",
    "# - Income: between 20,000 and 120,000\n",
    "# - Has bought before: yes or no\n",
    "# - Purchase: yes or no\n",
    "\n",
    "# We have two datasets, one with a very clear pattern and another completely random; this is useful to observe how prediction accuracy varies\n",
    "\n",
    "# Here we can see a very clear purchase pattern\n",
    "incomes = np.random.randint(20000, 100000, n)\n",
    "ages = np.random.randint(18, 70, n)\n",
    "has_bought_before = np.random.randint(0, 2, n)\n",
    "will_purchase = []\n",
    "for i in range(n):\n",
    "    if incomes[i] > 70000:\n",
    "        if ages[i] >= 40:\n",
    "            will_purchase.append(1)  # Yes\n",
    "        else:\n",
    "            will_purchase.append(0)  # No\n",
    "    else:\n",
    "        if has_bought_before[i] == 1:\n",
    "            will_purchase.append(1)  # Yes\n",
    "        else:\n",
    "            will_purchase.append(0)  # No\n",
    "            \n",
    "data = {\n",
    "    'Age': ages,\n",
    "    'Income': incomes,\n",
    "    'HasBoughtBefore': has_bought_before,\n",
    "    'WillPurchase': will_purchase\n",
    "}\n",
    "\n",
    "# Here we have a dataset with completely random values, the accuracy should drop significantly\n",
    "# incomes = np.random.randint(20000, 120000, n)\n",
    "# ages = np.random.randint(18, 70, n)\n",
    "# has_bought_before = np.random.choice([0, 1], n)\n",
    "# will_purchase = np.random.choice(['Yes', 'No'], n)\n",
    "\n",
    "# # Create DataFrame\n",
    "# data = {\n",
    "#     'Age': ages,\n",
    "#     'Income': incomes,\n",
    "#     'HasBoughtBefore': has_bought_before,\n",
    "#     'WillPurchase': will_purchase\n",
    "# }\n",
    "\n",
    "# Convert the dataset into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('market_segmentation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data\n",
    "data = pd.read_csv('market_segmentation.csv')\n",
    "\n",
    "# # Show the first few rows\n",
    "print(data.head())\n",
    "\n",
    "# # Descriptive statistics\n",
    "# # count: non-null values\n",
    "# # mean: average (sum of the values of each column divided by the number of rows)\n",
    "# # std: standard deviation\n",
    "# # min: minimum value per column\n",
    "# # 25%: the 25th percentile\n",
    "# # 50%: the 50th percentile (median)\n",
    "# # 75%: the 75th percentile\n",
    "# # max: maximum value per column\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purchase distribution\n",
    "plt.hist(data['WillPurchase'])\n",
    "plt.xlabel('WillPurchase')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Purchase Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "data['HasBoughtBefore'] = label_encoder.fit_transform(data['HasBoughtBefore'])\n",
    "data['WillPurchase'] = label_encoder.fit_transform(data['WillPurchase'])\n",
    "\n",
    "# Here we can see a graph where the x-axis shows the options ('Yes' and 'No')\n",
    "# The y-axis shows the frequency of each category in the data.\n",
    "# Frequency indicates how often each category appears in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to be considered for training the model\n",
    "X = df[['Age', 'Income', 'HasBoughtBefore']]\n",
    "y = df['WillPurchase']\n",
    "\n",
    "# Split into training and test sets\n",
    "# X variables to be used for prediction\n",
    "# y variable we want to predict\n",
    "# test_size=0.2: indicates that we will use 20% of the data for the test set and 80% for the training set\n",
    "# The training set is used to train the model, used to teach the model the relationship between patterns in the data (the more, the better)\n",
    "# The test set is used to compare the predictions and see how accurate they are\n",
    "# random_state: used to control how data is randomly split. If two people run the same function with the same value for random_state, \n",
    "# they will get exactly the same data split (test and training sets).\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_tree = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Decision Tree - Accuracy:\", accuracy_score(y_test, y_pred_tree))\n",
    "\n",
    "# Precision: The proportion of true positives over the total predicted positives (TP / (TP + FP)).\n",
    "# Recall: The proportion of true positives over the total actual positives (TP / (TP + FN)).\n",
    "# F1 Score: The harmonic mean of precision and recall, providing a balance between the two metrics (2 * (Precision * Recall) / (Precision + Recall)).\n",
    "# Support: The number of actual occurrences of the class in the data.\n",
    "print(\"Decision Tree - Classification Report:\\n\", classification_report(y_test, y_pred_tree))\n",
    "\n",
    "# Confusion matrix\n",
    "# TP (True Positives): Correct predictions where the model predicts the positive class correctly.\n",
    "# FP (False Positives): Incorrect predictions where the model predicts the positive class but the instance is negative.\n",
    "# FN (False Negatives): Incorrect predictions where the model predicts the negative class but the instance is positive.\n",
    "# TN (True Negatives): Correct predictions where the model predicts the negative class correctly.\n",
    "\n",
    "#                  Predicted Positive\t   Predicted Negative\n",
    "# Actual Positive\t       TP\t                       FN\n",
    "# Actual Negative\t       FP                        TN\n",
    "\n",
    "print(\"Decision Tree - Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import plot_tree\n",
    "\n",
    "# Visualize feature importance\n",
    "importances = model.feature_importances_\n",
    "features = X.columns\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "## In the following graph, we can see which features are the most important to consider\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Feature Importance')\n",
    "plt.bar(range(X.shape[1]), importances[indices], align='center')\n",
    "plt.xticks(range(X.shape[1]), [features[i] for i in indices])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
