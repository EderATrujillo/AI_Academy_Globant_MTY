{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention and Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Attention is all you need_ was the influential paper published by Google Research in 2017 that led to a whole new class of NLP models named \"Transformers\" and their derivatives.\n",
    "We will go through the stelar concept that is \"Self-attention\" that was used in this paper for the Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's the attention mechanism introduced in the paper \"Attention is all you need\"?\n",
    "\n",
    "Before delving into the different kinds of attention discussed on the paper, we need to review some concepts on embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Embeddings\n",
    "\n",
    "As discussed on the module on NLP, an _embedding_ is a mapping from a set of words or \"tokens\" to a high dimensional Euclidean space.\n",
    "In the world of Machine Learning a good embedding is one that **can encode semantical relationships between words** and whose axes **represent some quality or attribute of words** (e.g. sentiment, is it a big object, etc.). \n",
    "\n",
    "![Example of what an embedding might look like in three dimensions](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*SYiW1MUZul1NvL1kc1RxwQ.png)\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Tokens and words are not exactly the same thing, other than words, a token might be a class of punctuation symbols, a suffix or prefix of words or placeholders for words not in the training dataset. \n",
    "</div>\n",
    "\n",
    "Some of the most used tools for embeddings are [word2vec](https://code.google.com/archive/p/word2vec/) and [GloVe](https://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Queries, Keys and Values:\n",
    "\n",
    "The focal point of the Architecture are the multi-head attention blocks.\n",
    "\n",
    "![Transformer architecture](https://daleonai.com/images/screen-shot-2021-05-06-at-12.12.21-pm.png)\n",
    "\n",
    "To understand how they work we need to understand what do we mean by \"attention\". Let's take a look at the following formula:\n",
    "$$\n",
    "\\operatorname{Attention}(Q, K, V) = \\operatorname{softmax}(QK^T / \\sqrt{d_k})V\n",
    "$$\n",
    "This is known as _scaled dot product attention_. Let's break it down:\n",
    "\n",
    "\n",
    "- **Queries $Q$:** This is a vector of embeddings that represent what the model is looking for.\n",
    "- **Keys $K$:** Represent what is being attended to.\n",
    "- **Values $V$:** Represent the information associated with each key.\n",
    "- **dimension $d_k$**: This is the dimension of the keys vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** In a sentence translation task, the query might be a word in the target language, the keys might be words in the source language, and the values might be their corresponding translations. We'll implement it in simple Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37996893436819246, 0.3603853979421595, 0.2596456676896481]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Let's create a simple 2D embedding for a word-bag\n",
    "\n",
    "wordbag=['El', 'perro','tiene', 'el', 'pelo', 'suelto', 'The', 'dog', 'has', 'loose', 'hair']\n",
    "\n",
    "# For this example, we can ignore capitalization\n",
    "\n",
    "wordbag=[word.upper() for word in wordbag]\n",
    "\n",
    "def circle_embedding(dataset):\n",
    "    \"\"\"\n",
    "    Maps every element of the dataset to an element on the unit circle: list -> dict\n",
    "    \"\"\"\n",
    "    d_s=len(dataset)\n",
    "    embedding = dict(\n",
    "                 map(\n",
    "                     lambda item:\n",
    "                     (item[1], (math.cos(2*math.pi*item[0]/d_s),\n",
    "                                math.sin(2*math.pi*item[0]/d_s))),\n",
    "                     enumerate(dataset)))\n",
    "    return embedding\n",
    "\n",
    "embedding = circle_embedding(wordbag)\n",
    "\n",
    "# Let's create our Q,K and V. For this example we will translate 'Perro'. \n",
    "\n",
    "Q = embedding['PERRO']\n",
    "K = [embedding[i] for i in ['PERRO', 'TIENE', 'PELO']]\n",
    "V = [embedding[i] for i in ['DOG', 'HAS', 'HAIR']]\n",
    "\n",
    "def softmax(v):\n",
    "    \"\"\"\n",
    "    Simple softmax implementation for 1D vectors.\n",
    "    \"\"\"\n",
    "    exp_sum = sum([math.exp(e) for e in v])\n",
    "    for element in v:\n",
    "        yield math.exp(element)/exp_sum\n",
    "\n",
    "def dot_attention(Q, K, V):\n",
    "    d_k = len(K)\n",
    "    v = []\n",
    "\n",
    "    for k in K:\n",
    "        v.append((Q[0]*k[0] + Q[1]*k[1])/d_k)\n",
    "    s = softmax(v)\n",
    "    return s\n",
    "    #return map(lambda x: (x[0]*x[1][0], x[0]*x[1][1]), zip(s,V)) -> we return the softmax score for easier visualization of the probabilities.\n",
    "\n",
    "print(list(dot_attention(Q,K,V)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw earlier, attention by itself doesn't really have any learnable parameters, but it can be used for \n",
    "\n",
    "\n",
    "- **Weighting mechanism:** Assigns weights to different parts of the input sequence.\n",
    "- **Focus on relevant information:** Allows the model to focus on the most relevant parts of the input.\n",
    "- **Example:** In a machine translation task, the attention mechanism might focus on the part of the source sentence that is most relevant to the current word in the target sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Scaled dot product attention:**\n",
    "\n",
    "- **Calculation:** Similar to dot product attention, but includes a scaling factor.\n",
    "- **Formula:** Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V\n",
    "- **Benefits:** Prevents vanishing gradients for large values of d_k.\n",
    "\n",
    "### How did sequential RNN's worked and performed?\n",
    "\n",
    "**LSTM blocks:**\n",
    "\n",
    "- **Long Short-Term Memory:** A type of recurrent neural network (RNN) designed to address the vanishing gradient problem.\n",
    "- **Components:** Input gate, forget gate, output gate, cell state.\n",
    "- **Benefits:** Can learn long-range dependencies in sequential data.\n",
    "\n",
    "**Forgetfulness:**\n",
    "\n",
    "- **Vanishing gradient problem:** As information is processed through multiple layers, gradients can become very small, making it difficult for the model to learn long-range dependencies.\n",
    "- **Impact:** Limits the ability of RNNs to capture long-term dependencies.\n",
    "\n",
    "**Sequential RNN model:**\n",
    "\n",
    "- **Processing:** Processes input sequence one token at a time.\n",
    "- **Hidden states:** Stores information about the previous inputs.\n",
    "- **Limitations:** Can be computationally expensive for long sequences.\n",
    "\n",
    "### What's self-attention?\n",
    "\n",
    "**Embedding matrices:**\n",
    "\n",
    "- **Representations:** Convert words into numerical representations.\n",
    "- **Learning:** Learned from the training data.\n",
    "\n",
    "**Dot product as a way to measure similarity:**\n",
    "\n",
    "- **Calculation:** Calculates the dot product between queries and keys.\n",
    "- **Similarity measure:** The larger the dot product, the more similar the query and key.\n",
    "\n",
    "**Trained embeddings as a mechanism to encode text characteristics:**\n",
    "\n",
    "- **Contextual understanding:** Capture the meaning of words based on their context.\n",
    "- **Semantic relationships:** Learn to represent semantic relationships between words.\n",
    "\n",
    "**Masking in attention:**\n",
    "\n",
    "- **Prevention:** Prevents the model from attending to future tokens in the sequence.\n",
    "- **Causality:** Ensures that the model only considers information from the past.\n",
    "\n",
    "### What's multi-headed attention?\n",
    "\n",
    "**Projections as heads:**\n",
    "\n",
    "- **Parallel attention mechanisms:** Divide the input into multiple parallel attention mechanisms.\n",
    "- **Different perspectives:** Each head can focus on different aspects of the input.\n",
    "\n",
    "**Different heads encode different meanings:**\n",
    "\n",
    "- **Diverse understanding:** Can capture different relationships between words.\n",
    "- **Improved performance:** Enhances the model's ability to understand complex language patterns.\n",
    "\n",
    "**Embeddings as a way to derive meaning of word from context:**\n",
    "\n",
    "- **Contextual understanding:** Learn to capture the meaning of words based on their context.\n",
    "- **Semantic relationships:** Represent semantic relationships between words.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
